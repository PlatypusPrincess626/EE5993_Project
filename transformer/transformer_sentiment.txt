**Room for Improvement – Transformer Sentiment Model**

This file outlines planned or recommended changes to improve the performance of the custom Transformer sentiment analysis model trained on `emotion_dataset_raw.csv`.

---

### 1. Better Tokenization (Text-to-Sequence Quality)

* **Increase Vocabulary Size**: Raise `num_words` from 10,000 to 20,000 or 30,000 to capture more expressive vocabulary from \~35,000 samples.

* **Text Cleaning**:

  * Convert all text to lowercase.
  * Remove extra punctuation.
  * Normalize emojis and slang (optional, depending on preprocessing pipeline).

* **Noise Removal**:

  * Strip URLs.
  * Remove @mentions and hashtags if not informative for emotion prediction.

---

### 2. Training Duration

* **Increase Epochs**: Extend training from 10 to at least 20 epochs.
	reducing batch_size if memory or convergence is an issue.

---

### 3. Regularization and Generalization

* **Add Dropout**:

  * Apply `nn.Dropout(0.1–0.3)` in transformer layers.
  * Add dropout before final classifier layer.

* **Weight Decay**:

  * Set `weight_decay=0.01` in Adam optimizer to discourage overfitting.

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)
```

---

These updates are expected to improve both generalization and robustness of the model on real-world social media data.

---

*File maintained for repository documentation and iterative refinement.*



